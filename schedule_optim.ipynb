{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight scheduling optimization with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gymnasium.wrappers import NormalizeObservation, NormalizeReward\n",
    "from schedule_optimizer.model_evaluation import run_evaluation, RewardTracker\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from schedule_optimizer.flight_scheduling import FlightSchedulingEnv\n",
    "from schedule_optimizer.utils import generate_random_flight_schedule, generate_lambdas\n",
    "from schedule_optimizer.performance_tests import benchmark_polars_vs_numpy\n",
    "from schedule_optimizer.iterative_optimization import optimize_schedule\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from schedule_optimizer.flight_scheduling_optimized import FlightSchedulingEnvMasked\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple case : 2 connecting flights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate flight schedule and lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_0 = pl.DataFrame({\n",
    "    'departure' : [datetime(1900, 1, 1, 8, 0, 0), datetime(1900, 1, 1, 13, 30, 0)],\n",
    "    'arrival' : [datetime(1900, 1, 1, 10, 0, 0), datetime(1900, 1, 1, 14, 30, 0)],\n",
    "    'way' : [-1, 1],\n",
    "    'airport' : ['JFK', 'MAD'],\n",
    "    'departure_minutes' : [480, 810],\n",
    "    'arrival_minutes' : [600, 870],\n",
    "})\n",
    "lambdas = generate_lambdas(schedule_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial revenue : 875.0\n"
     ]
    }
   ],
   "source": [
    "env = FlightSchedulingEnv(\n",
    "    flight_schedule=schedule_0, \n",
    "    lambdas=lambdas, \n",
    "    max_steps=50,\n",
    "    revenue_estimation='classic',\n",
    "    obs_back=\"numpy\"\n",
    ")\n",
    "\n",
    "env = NormalizeObservation(env)\n",
    "check_env(env)\n",
    "env.reset()\n",
    "print(\"Initial revenue :\", env.env.calculate_revenue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Observation: [-0.5140178] Reward: 27.77777777777783\n",
      "2\n",
      "Observation: [-1.4600238] Reward: 27.777777777777715\n",
      "0\n",
      "Observation: [-0.3961249] Reward: -27.777777777777715\n",
      "0\n",
      "Observation: [0.6545185] Reward: -27.77777777777783\n",
      "3\n",
      "Observation: [1.5917014] Reward: -27.77777777777783\n",
      "2\n",
      "Observation: [0.53099865] Reward: 27.77777777777783\n",
      "4\n",
      "Observation: [0.51617306] Reward: 0.0\n",
      "0\n",
      "Observation: [1.4849986] Reward: -27.77777777777783\n",
      "1\n",
      "Observation: [0.4263473] Reward: 27.77777777777783\n",
      "0\n",
      "Observation: [1.4042425] Reward: -27.77777777777783\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    print(action)\n",
    "    print(\"Observation:\", obs, \"Reward:\", reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1000: Reward = -177.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f961c5579e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_TIMESTEPS = 2000\n",
    "EVAL_FREQ = 1000\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env,\n",
    "    learning_rate=0.003,\n",
    "    batch_size=32,\n",
    "    n_epochs=10,\n",
    "    clip_range=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "run_evaluation(TOTAL_TIMESTEPS, EVAL_FREQ, model, {}, env, masked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_schedule = generate_random_flight_schedule(50)\n",
    "random_lambdas = generate_lambdas(random_schedule)\n",
    "\n",
    "perfect_schedule = random_schedule.clone()\n",
    "\n",
    "perfect_schedule = perfect_schedule.with_columns(\n",
    "    departure_minutes = pl.when(pl.col('way') == -1)\n",
    "    .then(pl.lit(400)).otherwise(pl.lit(800)),\n",
    "    arrival_minutes = pl.when(pl.col('way') == -1)\n",
    "    .then(pl.lit(680)).otherwise(pl.lit(1000))\n",
    ")\n",
    "\n",
    "base_date = datetime(1900, 1, 1, 0, 0, 0)\n",
    "\n",
    "perfect_schedule = perfect_schedule.with_columns([\n",
    "    pl.lit(base_date).dt.offset_by(pl.col('departure_minutes').cast(pl.String) + \"m\").alias('departure'),\n",
    "    pl.lit(base_date).dt.offset_by(pl.col('arrival_minutes').cast(pl.String) + \"m\").alias('arrival')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial revenue : 117066.6666666667\n",
      "Perfect revenue : 423000.0\n"
     ]
    }
   ],
   "source": [
    "env = FlightSchedulingEnv(\n",
    "    flight_schedule=random_schedule,\n",
    "    lambdas=random_lambdas,\n",
    "    max_steps=50,\n",
    "    revenue_estimation='classic'\n",
    ")\n",
    "perfect_env = FlightSchedulingEnv(\n",
    "    flight_schedule=perfect_schedule,\n",
    "    lambdas=random_lambdas,\n",
    "    max_steps=50,\n",
    "    revenue_estimation='classic'\n",
    ")\n",
    "env = NormalizeObservation(env)\n",
    "check_env(env)\n",
    "\n",
    "print(\"Initial revenue :\", env.env.calculate_revenue())\n",
    "print(\"Perfect revenue :\", perfect_env.calculate_revenue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis avec Timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Benchmark Polars vs Numpy pour calcul de revenue...\n",
      "   ğŸ”¹ Test avec Polars...\n",
      "   ğŸ”¹ Test avec Numpy...\n",
      "   âœ… Polars: 0.0743s total, 14.85ms/calcul\n",
      "   âœ… Numpy:  0.0041s total, 0.82ms/calcul\n",
      "   ğŸ† Gagnant: Numpy (speedup: 18.16x)\n",
      "ğŸ“Š RÃ©sultats benchmark:\n",
      "   Gagnant: Numpy\n",
      "   Speedup: 18.16x\n"
     ]
    }
   ],
   "source": [
    "small_schedule = generate_random_flight_schedule(100)\n",
    "small_lambdas = generate_lambdas(small_schedule)\n",
    "\n",
    "benchmark_results = benchmark_polars_vs_numpy(\n",
    "    small_schedule, small_lambdas, num_calculations=5\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š RÃ©sultats benchmark:\")\n",
    "print(f\"   Gagnant: {benchmark_results['winner']}\")\n",
    "print(f\"   Speedup: {benchmark_results['speedup_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ EntraÃ®nement avec Action Masking...\n",
      "  Step 1000: Reward =   0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x7f95ae8400e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_base = FlightSchedulingEnvMasked(\n",
    "    flight_schedule=schedule_0,\n",
    "    lambdas=lambdas,\n",
    "    max_steps=50,\n",
    "    revenue_estimation='classic',\n",
    "    obs_back=\"numpy\"\n",
    ")\n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.get_action_mask()\n",
    "\n",
    "env = ActionMasker(env_base, mask_fn)\n",
    "env = NormalizeObservation(env)\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env,\n",
    "    learning_rate=0.003,\n",
    "    batch_size=32,\n",
    "    n_epochs=10,\n",
    "    clip_range=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ EntraÃ®nement avec Action Masking...\")\n",
    "run_evaluation(TOTAL_TIMESTEPS, EVAL_FREQ, model, {}, env, masked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation ItÃ©rative avec Extraction de Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ OPTIMISATION ITÃ‰RATIVE AVEC MULTI-SAMPLING - 20 itÃ©rations\n",
      "ğŸ”¬ ParamÃ¨tres: 50 Ã©chantillons par extraction\n",
      "======================================================================\n",
      "ğŸ”„ ITÃ‰RATION 1\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -27874.72\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=17155.56, revenue=145751.39, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 128595.83 â†’ 145751.39 (+17155.56, +13.3%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 47.2s\n",
      "   ğŸ† Meilleur revenue global: 145751.39\n",
      "ğŸ”„ ITÃ‰RATION 2\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -24410.28\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=10809.72, revenue=156561.11, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 145751.39 â†’ 156561.11 (+10809.72, +7.4%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 47.7s\n",
      "   ğŸ† Meilleur revenue global: 156561.11\n",
      "ğŸ”„ ITÃ‰RATION 3\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -49940.83\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=12223.61, revenue=168784.72, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 156561.11 â†’ 168784.72 (+12223.61, +7.8%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 50.5s\n",
      "   ğŸ† Meilleur revenue global: 168784.72\n",
      "ğŸ”„ ITÃ‰RATION 4\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -39905.00\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=11613.89, revenue=180398.61, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 168784.72 â†’ 180398.61 (+11613.89, +6.9%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 46.7s\n",
      "   ğŸ† Meilleur revenue global: 180398.61\n",
      "ğŸ”„ ITÃ‰RATION 5\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -49980.28\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=16468.06, revenue=196866.67, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 180398.61 â†’ 196866.67 (+16468.06, +9.1%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 48.0s\n",
      "   ğŸ† Meilleur revenue global: 196866.67\n",
      "ğŸ”„ ITÃ‰RATION 6\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -75150.00\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=15065.28, revenue=211931.94, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 196866.67 â†’ 211931.94 (+15065.28, +7.7%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 47.9s\n",
      "   ğŸ† Meilleur revenue global: 211931.94\n",
      "ğŸ”„ ITÃ‰RATION 7\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -47295.83\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=-1877.78, revenue=210054.17, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âŒ REJETÃ‰ Revenue: 211931.94 â†’ 210054.17 (-1877.78, -0.9%)\n",
      "   ğŸ”„ Maintien du planning actuel\n",
      "   â±ï¸  Temps d'entraÃ®nement: 48.7s\n",
      "   ğŸ† Meilleur revenue global: 211931.94\n",
      "ğŸ”„ ITÃ‰RATION 8\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -44362.78\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=17433.33, revenue=229365.28, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 211931.94 â†’ 229365.28 (+17433.33, +8.2%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 57.1s\n",
      "   ğŸ† Meilleur revenue global: 229365.28\n",
      "ğŸ”„ ITÃ‰RATION 9\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -50862.78\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=28979.17, revenue=258344.44, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 229365.28 â†’ 258344.44 (+28979.17, +12.6%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 48.0s\n",
      "   ğŸ† Meilleur revenue global: 258344.44\n",
      "ğŸ”„ ITÃ‰RATION 10\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -78023.61\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=519.44, revenue=258863.89, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 258344.44 â†’ 258863.89 (+519.44, +0.2%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 48.9s\n",
      "   ğŸ† Meilleur revenue global: 258863.89\n",
      "ğŸ”„ ITÃ‰RATION 11\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -86440.56\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=-5930.56, revenue=252933.33, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âŒ REJETÃ‰ Revenue: 258863.89 â†’ 252933.33 (-5930.56, -2.3%)\n",
      "   ğŸ”„ Maintien du planning actuel\n",
      "   â±ï¸  Temps d'entraÃ®nement: 49.3s\n",
      "   ğŸ† Meilleur revenue global: 258863.89\n",
      "ğŸ”„ ITÃ‰RATION 12\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -80795.83\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=876.39, revenue=259740.28, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âœ… ACCEPTÃ‰ Revenue: 258863.89 â†’ 259740.28 (+876.39, +0.3%)\n",
      "   â±ï¸  Temps d'entraÃ®nement: 48.4s\n",
      "   ğŸ† Meilleur revenue global: 259740.28\n",
      "ğŸ”„ ITÃ‰RATION 13\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -63351.94\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=-941.67, revenue=258798.61, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âŒ REJETÃ‰ Revenue: 259740.28 â†’ 258798.61 (-941.67, -0.4%)\n",
      "   ğŸ”„ Maintien du planning actuel\n",
      "   â±ï¸  Temps d'entraÃ®nement: 49.1s\n",
      "   ğŸ† Meilleur revenue global: 259740.28\n",
      "ğŸ”„ ITÃ‰RATION 14\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -76488.61\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=-566.67, revenue=259173.61, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âŒ REJETÃ‰ Revenue: 259740.28 â†’ 259173.61 (-566.67, -0.2%)\n",
      "   ğŸ”„ Maintien du planning actuel\n",
      "   â±ï¸  Temps d'entraÃ®nement: 49.9s\n",
      "   ğŸ† Meilleur revenue global: 259740.28\n",
      "ğŸ”„ ITÃ‰RATION 15\n",
      "   Timesteps par itÃ©ration: 20000\n",
      "   Ã‰chantillons d'extraction: 50\n",
      "   ğŸš€ EntraÃ®nement en cours...\n",
      "   ğŸ“Š Ã‰valuation post-entraÃ®nement: -76933.61\n",
      "ğŸ” Extraction du planning optimisÃ© avec 50 Ã©chantillons...\n",
      "   âœ… MEILLEUR Ã©chantillon: reward=-6701.39, revenue=253038.89, steps=501\n",
      "   ğŸ“ˆ AmÃ©lioration sur 50 essais\n",
      "   âŒ REJETÃ‰ Revenue: 259740.28 â†’ 253038.89 (-6701.39, -2.6%)\n",
      "   ğŸ”„ Maintien du planning actuel\n",
      "   â±ï¸  Temps d'entraÃ®nement: 51.5s\n",
      "   ğŸ† Meilleur revenue global: 259740.28\n",
      "âš ï¸  Convergence dÃ©tectÃ©e : 3 rejets consÃ©cutifs.\n",
      "   ArrÃªt anticipÃ© Ã  l'itÃ©ration 15.\n",
      "ğŸ“Š RÃ‰SUMÃ‰ OPTIMISATION ITÃ‰RATIVE AVEC MULTI-SAMPLING\n",
      "ğŸ”¬ Configuration: 50 Ã©chantillons par extraction\n",
      "======================================================================\n",
      "Nombre d'itÃ©rations: 15\n",
      "ItÃ©rations acceptÃ©es: 10 (66.7%)\n",
      "ItÃ©rations rejetÃ©es: 5 (33.3%)\n",
      "Total timesteps: 300,000\n",
      "Temps total: 739.0s\n",
      "ğŸ“ˆ Ã‰VOLUTION DU REVENUE:\n",
      "  ItÃ©ration  1: 128595.83 â†’ 145751.39 (+17155.56, +13.3%) âœ… eval=-27874.7\n",
      "  ItÃ©ration  2: 145751.39 â†’ 156561.11 (+10809.72,  +7.4%) âœ… eval=-24410.3\n",
      "  ItÃ©ration  3: 156561.11 â†’ 168784.72 (+12223.61,  +7.8%) âœ… eval=-49940.8\n",
      "  ItÃ©ration  4: 168784.72 â†’ 180398.61 (+11613.89,  +6.9%) âœ… eval=-39905.0\n",
      "  ItÃ©ration  5: 180398.61 â†’ 196866.67 (+16468.06,  +9.1%) âœ… eval=-49980.3\n",
      "  ItÃ©ration  6: 196866.67 â†’ 211931.94 (+15065.28,  +7.7%) âœ… eval=-75150.0\n",
      "  ItÃ©ration  7: 211931.94 â†’ 210054.17 (-1877.78,  -0.9%) âŒ eval=-47295.8\n",
      "  ItÃ©ration  8: 211931.94 â†’ 229365.28 (+17433.33,  +8.2%) âœ… eval=-44362.8\n",
      "  ItÃ©ration  9: 229365.28 â†’ 258344.44 (+28979.17, +12.6%) âœ… eval=-50862.8\n",
      "  ItÃ©ration 10: 258344.44 â†’ 258863.89 (+519.44,  +0.2%) âœ… eval=-78023.6\n",
      "  ItÃ©ration 11: 258863.89 â†’ 252933.33 (-5930.56,  -2.3%) âŒ eval=-86440.6\n",
      "  ItÃ©ration 12: 258863.89 â†’ 259740.28 (+876.39,  +0.3%) âœ… eval=-80795.8\n",
      "  ItÃ©ration 13: 259740.28 â†’ 258798.61 (-941.67,  -0.4%) âŒ eval=-63351.9\n",
      "  ItÃ©ration 14: 259740.28 â†’ 259173.61 (-566.67,  -0.2%) âŒ eval=-76488.6\n",
      "  ItÃ©ration 15: 259740.28 â†’ 253038.89 (-6701.39,  -2.6%) âŒ eval=-76933.6\n",
      "ğŸ¯ AMÃ‰LIORATION TOTALE:\n",
      "   Initial: 128595.83\n",
      "   Meilleur: 259740.28\n",
      "   Gain:    +131144.44 (+102.0%)\n",
      "   ROI:     177.457 revenue/seconde\n",
      "   EfficacitÃ©: 13114.44 revenue/itÃ©ration acceptÃ©e\n",
      "ğŸ“Š MÃ©triques d'optimisation:\n",
      "   AmÃ©lioration: 102.0%\n",
      "   Revenue initial: 128595.83\n",
      "   Revenue final: 259740.28\n",
      "   Temps total: 739.0s\n"
     ]
    }
   ],
   "source": [
    "optimized_schedule, metrics = optimize_schedule(\n",
    "    random_schedule,\n",
    "    random_lambdas,\n",
    "    num_iterations=20,\n",
    "    timesteps_per_iteration=20000,\n",
    "    n_samples=50,\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š MÃ©triques d'optimisation:\")\n",
    "print(f\"   AmÃ©lioration: {metrics.get('improvement_percentage', 0):.1f}%\")\n",
    "print(f\"   Revenue initial: {metrics.get('initial_revenue', 0):.2f}\")\n",
    "print(f\"   Revenue final: {metrics.get('best_revenue', 0):.2f}\")\n",
    "print(f\"   Temps total: {metrics.get('total_training_time', 0):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre d'itÃ©rations: 10\n",
    "ItÃ©rations acceptÃ©es: 9 (90.0%)\n",
    "ItÃ©rations rejetÃ©es: 1 (10.0%)\n",
    "Total timesteps: 200,000\n",
    "Temps total: 460.4s\n",
    "ğŸ“ˆ Ã‰VOLUTION DU REVENUE:\n",
    "  ItÃ©ration  1: 128595.83 â†’ 138904.17 (+10308.33,  +8.0%) âœ… eval=-16875.6\n",
    "  ItÃ©ration  2: 138904.17 â†’ 149523.61 (+10619.44,  +7.6%) âœ… eval=-27053.9\n",
    "  ItÃ©ration  3: 149523.61 â†’ 160704.17 (+11180.56,  +7.5%) âœ… eval=-27380.0\n",
    "  ItÃ©ration  4: 160704.17 â†’ 178912.50 (+18208.33, +11.3%) âœ… eval=-16952.8\n",
    "  ItÃ©ration  5: 178912.50 â†’ 194718.06 (+15805.56,  +8.8%) âœ… eval=-41305.6\n",
    "  ItÃ©ration  6: 194718.06 â†’ 201286.11 (+6568.06,  +3.4%) âœ… eval=-61890.6\n",
    "  ItÃ©ration  7: 201286.11 â†’ 217605.56 (+16319.44,  +8.1%) âœ… eval=-31476.4\n",
    "  ItÃ©ration  8: 217605.56 â†’ 224318.06 (+6712.50,  +3.1%) âœ… eval=-65282.8\n",
    "  ItÃ©ration  9: 224318.06 â†’ 219230.56 (-5087.50,  -2.3%) âŒ eval=-65468.1\n",
    "  ItÃ©ration 10: 224318.06 â†’ 226000.00 (+1681.94,  +0.7%) âœ… eval=-16932.5\n",
    "ğŸ¯ AMÃ‰LIORATION TOTALE:\n",
    "   Initial: 128595.83\n",
    "   Meilleur: 226000.00\n",
    "   Gain:    +97404.17 (+75.7%)\n",
    "   ROI:     211.583 revenue/seconde\n",
    "   EfficacitÃ©: 10822.69 revenue/itÃ©ration acceptÃ©e\n",
    "ğŸ“Š MÃ©triques d'optimisation:\n",
    "   AmÃ©lioration: 75.7%\n",
    "   Revenue initial: 128595.83\n",
    "   Revenue final: 226000.00\n",
    "   Temps total: 460.4s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-schedule-optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
